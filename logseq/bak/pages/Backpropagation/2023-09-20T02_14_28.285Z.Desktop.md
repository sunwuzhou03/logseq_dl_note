# Gradient Descent
	- There are millions of parameters. To compute the gradients efficiently, we use backpropagation.
	  collapsed:: true
		- ![image.png](../assets/image_1694274523267_0.png)
- # Chain Rule
  collapsed:: true
	- examples
		- ![image.png](../assets/image_1694274659601_0.png)
- # Backpropagation
  collapsed:: true
	- ![image.png](../assets/image_1694274909668_0.png){:height 398, :width 520}
	- calculate partial
		- compute $$\frac{\partial C}{\partial w}$$, $$y$$ is the output layer neuron network output, $$a$$ is the output of hidden layer.
		  collapsed:: true
			- ![image.png](../assets/image_1694275018318_0.png){:height 524, :width 689}
		- forward pass, compute $$\frac{\partial z}{\partial w}$$, this is very easy.
		  collapsed:: true
			- ![image.png](../assets/image_1694275352125_0.png) 
			  ![image.png](../assets/image_1694275231164_0.png)
		- backward pass, compute $$\frac{\partial C}{\partial z}$$
		  collapsed:: true
			- The $$a$$ affects $$C$$ through $$z^{'}$$ and $$z^{''}$$
			  collapsed:: true
				- ![image.png](../assets/image_1694275500414_0.png)
				  ![image.png](../assets/image_1694275563327_0.png)
			- the above formula can be think in another view
			  collapsed:: true
				- ![image.png](../assets/image_1694275708830_0.png)
			- case 1: only have one hidden layer
			  collapsed:: true
				- ![image.png](../assets/image_1694275849747_0.png)
			- case 2
			  collapsed:: true
				- ![image.png](../assets/image_1694276056428_0.png)
				  ![image.png](../assets/image_1694276227687_0.png)
				  ![image.png](../assets/image_1694276334967_0.png)