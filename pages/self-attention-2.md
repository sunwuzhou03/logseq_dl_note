- ![image.png](../assets/image_1702047831947_0.png)
- # how to use the $$a^i$$ to compute the $$b^i$$
  collapsed:: true
	- ![image.png](../assets/image_1702047988354_0.png)
	- ![image.png](../assets/image_1702048190523_0.png)
	- ![image.png](../assets/image_1702048424250_0.png)
	- ![image.png](../assets/image_1702048569563_0.png)
	-
- ![image.png](../assets/image_1702048700272_0.png)
- # Multi-Head self-attention
	- ![image.png](../assets/image_1702048931623_0.png)
	- ![image.png](../assets/image_1702048950397_0.png)
- # positional encoding
	- ![image.png](../assets/image_1702049228125_0.png)
	- ![image.png](../assets/image_1702049308022_0.png)
	- we don't need to give a huge windows to use self-attention
	- the position encoding is also a problem to be solved
	- ![image.png](../assets/image_1702049371163_0.png)
	- ![image.png](../assets/image_1702049486045_0.png)
	- ![image.png](../assets/image_1702049619015_0.png)
	- ![image.png](../assets/image_1702049750999_0.png)
	- ![image.png](../assets/image_1702049781689_0.png)
	- ![image.png](../assets/image_1702049967651_0.png)
	- ![image.png](../assets/image_1702050267630_0.png)
	- ![image.png](../assets/image_1702050465782_0.png)
- # to learn more
	- ![image.png](../assets/image_1702050564194_0.png)
	-
-