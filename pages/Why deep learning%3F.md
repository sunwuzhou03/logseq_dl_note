- ![image.png](../assets/image_1701702676711_0.png)
- # Why hidden layer?
  collapsed:: true
	- ![image.png](../assets/image_1701703049757_0.png)
	- ![image.png](../assets/image_1701703229855_0.png)
	- ![image.png](../assets/image_1701703249181_0.png)
	- ![image.png](../assets/image_1701703382586_0.png)
	- two relu can combine the hard sigmoid
- # Fat Neural network
  collapsed:: true
	- ![image.png](../assets/image_1701703476527_0.png)
	- ![image.png](../assets/image_1701704016270_0.png)
	- ![image.png](../assets/image_1701704189421_0.png)
	- ![image.png](../assets/image_1701704292219_0.png)
- # Why we need deep?
  collapsed:: true
	- ![image.png](../assets/image_1701704452638_0.png)
	- for a same function, the deep model parameters is less than fat network.
	-
- # Use Analogy to explain deep model is better
  collapsed:: true
	- ![image.png](../assets/image_1701704707071_0.png)
	- ![image.png](../assets/image_1701704787093_0.png)
	- ![image.png](../assets/image_1701704883464_0.png)
	-
-